# REPORT_Examining_biases_in_LLMs
This report investigates whether large language models (LLMs) exhibit biases when generating socio-economic data based on different racial identities. Using a standardized prompt and controlled variables, we analyze the potential disparities in LLM outputs. Explore our findings and the implications for fairness in AI.
