![Image Description](https://github.com/Tyler-Gustafson/Examining_biases_in_LLMs_causal_analysis/blob/main/01_background/title_page.jpg?raw=true)

This report investigates whether large language models (LLMs) exhibit biases when generating socio-economic data based on different racial identities. Using a standardized prompt and controlled variables, we analyze the potential disparities in LLM outputs. Explore our findings and the implications for fairness in AI.

### Authors
[Tyler Gustafson](https://www.linkedin.com/in/tylergustafson/)

[Assaph (Safi) Aharoni](https://www.linkedin.com/in/assaph-aharoni/)

[Alexandra Daniels](https://www.ischool.berkeley.edu/people/alexandra-daniels)

[Conner Davis](https://www.linkedin.com/in/connerdavis/)

# Contents

[Full Write Up](https://www.tylerjaygustafson.com/research-bias-in-llms)
