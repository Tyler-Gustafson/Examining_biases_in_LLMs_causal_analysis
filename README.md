![Image Description](https://github.com/Tyler-Gustafson/Examining_biases_in_LLMs_causal_analysis/blob/main/01_background/title_page.jpg?raw=true)

This report investigates whether large language models (LLMs) exhibit biases when generating socio-economic data based on different racial identities. Using a standardized prompt and controlled variables, we analyze the potential disparities in LLM outputs. Explore our findings and the implications for fairness in AI.

### Authors / Contributors
<a href="https://www.linkedin.com/in/tylergustafson/" target="_blank">Tyler Gustafson</a>

<a href="https://www.linkedin.com/in/assaph-aharoni/" target="_blank">Assaph (Safi) Aharoni</a>

<a href="https://www.ischool.berkeley.edu/people/alexandra-daniels" target="_blank">Alexandra Daniels</a>

<a href="https://www.linkedin.com/in/connerdavis/" target="_blank">Conner Davis</a>

# Contents

[Full Write Up](https://www.tylerjaygustafson.com/research-bias-in-llms)
[Full Report](https://github.com/Tyler-Gustafson/Examining_biases_in_LLMs_causal_analysis/blob/main/Final_Research_Report_Caual_Analysis_of_Biases_in_LLMs.pdf)






